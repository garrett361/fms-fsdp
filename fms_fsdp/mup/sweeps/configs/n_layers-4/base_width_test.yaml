# Train two models with same d_model, one without mup and one with mup an mup_base_d_model=d_model
# Training should be identical for all lrs.
name: transformer-only-n_layer-4-base_width_test
method: grid
metric:
  goal: minimize
  name: loss
parameters:
  file_type:
    value: auto
  col_name:
    value: contents
  tokenizer_path:
    value: /datasets/tokenizers/llama3
  weights:
    value: 1
  vocab_size:
    value: 128256
  bos_token:
    value: null
  eos_token:
    value: 128000
  use_torch_compile:
    value: False
  data_path:
    value: /datasets/dolma_v1_7
  datasets:
    value: dataset=cc_en_head
  tracker:
    value: wandb
  n_layer:
    value: 4
  d_model:
    values:
      - 512
      # - 1024
      # - 1536
      # - 2048
      # - 2560
      # - 3072
  # 2**20 tok batch size: seq_len = 4096 = 2**12, batch_size * acc_steps = 256 = 2**8
  seq_length:
    value: 4096
  batch_size:
    value: 4
  acc_steps:
    value: 64
  num_steps:
    value: 1024
  report_interval:
    value: 64
  mup:
    values:
    - true
    - false
  mup_base_d_model:
    value: 512
  # LRs: log-spaced between 1e-2 and 1e-5, inclusive. [f"{10**(-n/3):.3e}" for n in range(6, 16)]
  learning_rate:
    values:
      - 1.000e-02
      - 4.642e-03
      - 2.154e-03
      - 1.000e-03
      - 4.642e-04
      - 2.154e-04
      - 1.000e-04
      - 4.642e-05
      - 2.154e-05
      - 1.000e-05
