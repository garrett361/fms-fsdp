program: fms_fsp/mup/sweeps/run_sweep_worker.py
method: grid
metric:
  goal: minimize
  name: loss
parameters:
  file_type:
    value: auto
  col_name:
    value: contents
  tokenizer_path:
    value: /datasets/tokenizers/llama3
  weights:
    value: 1
  vocab_size:
    value: 128256
  bos_token:
    value: None
  eos_token:
    value: 128000
  use_torch_compile:
    value: False
  data_path:
    value: /datasets/dolma_v1_7
  datasets:
    value: dataset=cc_en_head
  tracker:
    value: wandb
  n_layer:
    value: 4
  width:
    value: 512
  # 2**20 tok batch size: seq_len = 4096, batch_size * acc_steps = 256
  seq_length:
    value: 4096
  batch_size:
    value: 4
  acc_steps:
    value: 64
  num_steps:
    value: 10000
  report_interval:
    value: 100
  # LRs: log-spaced between 1e-2 and 1e-5, inclusive. [10**(-n/3) for n in range(6, 16)]
  learning_rate:
    values: [0.01, 0.004641588833612777, 0.0021544346900318843, 0.001, 0.00046415888336127773, 0.00021544346900318845, 0.0001, 4.641588833612782e-05, 2.1544346900318823e-05, 1e-05]
